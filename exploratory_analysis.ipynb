{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IO\n",
    "from pathlib import Path\n",
    "\n",
    "# Utility Libraries\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "# Data Processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Feature Engineering\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n",
    "\n",
    "# Predictive Analytics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from bcpn_pipeline import data, features, models, consts\n",
    "\n",
    "# Viz\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "# sns.set_style(\"whitegrid\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'figure.autolayout': True})\n",
    "# plt.rcParams.update({'figure.facecolor': [1.0, 1.0, 1.0, 1.0]})\n",
    "\n",
    "# configure autoreloading of modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "datafile = Path.joinpath(consts.DATA_PATH, 'final_merged_set_v6.csv')\n",
    "df = pd.read_csv(datafile, parse_dates=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning & Feature Engineering\n",
    "Thank you to Jason Brownlee\n",
    "https://machinelearningmastery.com/basic-data-cleaning-for-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Dataset class\n",
    "dataset = data.Dataset(df, id_col = 'PtID')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Perform an initial cleaning of the dataset ----------\n",
    "dataset.clean(to_rename = {**consts.RENAMINGS['demographics'], \n",
    "                             **consts.RENAMINGS['medical']}, \n",
    "              to_drop=[col for col in dataset.df.columns if '_Name' in col] + \n",
    "                      ['MemsNum', 'Monitor', 'pre_dx_date'],\n",
    "              to_map = consts.CODEBOOK,\n",
    "              to_binarize = ['race_other'],\n",
    "              onehots_to_reverse = ['race_']\n",
    "             )\n",
    "\n",
    "''' Set dtypes on remaining columns\n",
    "For now, naively assume we only have numerics, datetimes, or objects\n",
    "'''\n",
    "dtypes_dict = {\n",
    "    'numeric': [col for col in dataset.df.columns if 'date' not in col.lower()],\n",
    "    'datetime': ['DateEnroll'],\n",
    "    'categorical': list(set(list(consts.CODEBOOK.keys()) + \\\n",
    "                            ['race', 'education', 'birth_country',\n",
    "                               'marital_status', 'employment', 'income',\n",
    "                               'primary_language', 'race',\n",
    "                               'stage'\n",
    "                            ]\n",
    "                           )\n",
    "                       )\n",
    "    }\n",
    "\n",
    "dataset.set_dtypes(dtypes_dict)\n",
    "dataset.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate static (non-temporal) features from measures such as validate instruments (e.g., FACTB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organize candidate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    " Organize the candidate features into useful categories for later reference\n",
    " A bit tedious, but helpful \n",
    "'''\n",
    "feat_categories = {\n",
    "    'demographics': [v for v in consts.RENAMINGS['demographics'].values()\n",
    "                     if v in dataset.df.columns] + ['race'], #add the new, single race col\n",
    "    'medical': [v for v in consts.RENAMINGS['medical'].values() if v in dataset.df.columns] + \\\n",
    "               [col for col in ['early_late', 'diagtoenroll'] \n",
    "                if col in dataset.df.columns],\n",
    "    'scores': []\n",
    "}\n",
    "feat_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This dataset has several repeated measures for validated instruments, \n",
    "such as the FACTB\n",
    "\n",
    "Columns for repeated measures for the same instrument share a suffix (e.g., '_FACTB')\n",
    "Use regex to populate the `scores` category subdictionary quickly, using these suffixes\n",
    "''' \n",
    "\n",
    "for k,v in consts.SCORES.items():\n",
    "    \n",
    "    #  Handle special case of BCPT before doing anything else\n",
    "    if k == 'BCPT':\n",
    "        dataset.df.drop(\n",
    "            list(dataset.df.filter(regex = '_BCPT\\d*YN$')), \n",
    "            axis = 1, \n",
    "            inplace = True\n",
    "        )\n",
    "        dataset.df.drop(\n",
    "            list(dataset.df.filter(regex = '_BCPT\\d*O$')), \n",
    "            axis = 1, \n",
    "            inplace = True\n",
    "        )\n",
    "    for prefix in consts.SCORE_PREFIXES:\n",
    "        # Some measures weren't precalculated. Let's fix this.  \n",
    "        if v['precalculated'] == False:\n",
    "\n",
    "            #  Get the aggregate score and add it to the dataset as a new column\n",
    "\n",
    "                score_cols = list(\n",
    "                    dataset.df.filter(regex='^' + prefix + v['suffix'] + '\\d*').columns\n",
    "                )\n",
    "\n",
    "                dataset.df[prefix + v['suffix']] = dataset.df[score_cols].sum(axis=1)\n",
    "\n",
    "        #  We'll include this new column as a feature\n",
    "        feat_categories['scores'] += [prefix + v['suffix']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Create a catch-all category of remaining features, to ensure we got everything '''\n",
    "other_feats = [col for col in dataset.df.columns \n",
    "              if col not in list(itertools.chain(*feat_categories.values())) # exclude anything already in the list\n",
    "              and not any(prefix in col for prefix in ['A_', 'B_', 'C_']) # exclude individual score cols\n",
    "              and 'date' not in col \n",
    "              and col not in dataset.id_col\n",
    "             ]\n",
    "\n",
    "other_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Create new columns for demographic and medical variables\n",
    "Be sure we update the feature categories dictionary '''\n",
    "\n",
    "demog_drug_cols = [col for col in dataset.df.columns if 'A_DEMO13DRUG' in col]\n",
    "newcol = 'DEMOG_numdrugs'\n",
    "dataset.df[newcol] = dataset.df[demog_drug_cols].count(axis=1)\n",
    "feat_categories['demographics'] += [newcol]\n",
    "\n",
    "# Removed post-exam cols - DUH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create featuresets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_featuresets = list()\n",
    "\n",
    "'''Create two distinct featuresets - one with demographics + med record data,\n",
    "   and one with scores '''\n",
    "fs_combos = [['demographics', 'medical'],['scores']]\n",
    "\n",
    "for combo in fs_combos:\n",
    "    feat_cats_subset = {k:v for k,v in feat_categories.items() if k in combo}\n",
    "    df = data.build_df_from_feature_categories(dataset.df, feat_cats_subset, dataset.id_col)\n",
    "    \n",
    "    # Add DateEnroll back\n",
    "    df = df.merge(dataset.df[['PtID', 'DateEnroll']], on=['PtID'], how='left')\n",
    "\n",
    "    ''' We have scores at baseline (0 months), mid-study (4 months) and post-study (8 months).\n",
    "        Here, we are going to reshape the data so dataframe is long-form and retain the 0 and 4 month scores.\n",
    "        \n",
    "        We are also going to set the time horizon column (e.g., study_day, study week) to be \n",
    "        the date we want to predict adherence (the next immediate time step)\n",
    "    '''\n",
    "    if 'scores' in combo:\n",
    "        dfs = []\n",
    "        \n",
    "        for prefix in consts.SCORE_PREFIXES:\n",
    "            i = consts.SCORE_PREFIXES.index(prefix)\n",
    "            df2 = df[['PtID', 'DateEnroll'] + [col for col in df.columns if prefix in col]]\n",
    "            df2.columns = df2.columns.str.replace(prefix, '')\n",
    "\n",
    "            if i == 0:\n",
    "                df2['study_day'] = (df2['DateEnroll'] + np.timedelta64(1, 'M') - df2['DateEnroll']).dt.days + 1\n",
    "                df2['study_week'] = np.floor(\n",
    "                    (df2['DateEnroll'] + np.timedelta64(1, 'M') - df2['DateEnroll']).dt.days / consts.DAYS_IN_WEEK\n",
    "                )\n",
    "                df2['study_month'] = 1\n",
    "                dfs.append(df2)\n",
    "                \n",
    "            elif i == 1: # Prefix B_, the 4-month surveys\n",
    "                df2['study_day'] = (df2['DateEnroll'] + np.timedelta64(4, 'M') - df2['DateEnroll']).dt.days\n",
    "                df2['study_week'] = np.floor(\n",
    "                    (df2['DateEnroll'] + np.timedelta64(4, 'M') - df2['DateEnroll']).dt.days / consts.DAYS_IN_WEEK\n",
    "                )\n",
    "                df2['study_month'] = 5\n",
    "                dfs.append(df2)\n",
    "                \n",
    "#             else: # Prefix C_, the 8-month surveys; predict into the past\n",
    "#                 df2['study_day'] = 239\n",
    "#                 df2['study_month'] = 7  \n",
    "    \n",
    "        df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "        \n",
    "        ''' Remove participants who didn't complete all questionnaires \n",
    "        Thank you Wes McKinney\n",
    "        https://stackoverflow.com/questions/14016247/find-integer-index-of-rows-with-nan-in-pandas-dataframe\n",
    "        '''\n",
    "        idx = pd.isnull(df).any(1).to_numpy().nonzero()[0]\n",
    "        ptids_to_exclude = list(df.iloc[idx, :]['PtID'].unique())\n",
    "        df = df.loc[~df['PtID'].isin(ptids_to_exclude)]\n",
    "    \n",
    "    else:\n",
    "        df['study_day'] = (df['DateEnroll'] + np.timedelta64(1, 'M') - df['DateEnroll']).dt.days + 1\n",
    "        df['study_week'] = np.floor(\n",
    "            (df['DateEnroll'] + np.timedelta64(1, 'M') - df['DateEnroll']).dt.days / consts.DAYS_IN_WEEK\n",
    "        )\n",
    "        df['study_month'] = 1\n",
    "    \n",
    "    df.drop(columns=['DateEnroll'], inplace=True)\n",
    "    static_featuresets.append(features.Featureset(df=df, name=' + '.join(combo), id_col = dataset.id_col))\n",
    "        \n",
    "static_featuresets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dem_med = static_featuresets[0].df.drop(columns=consts.TARGET_HORIZONS) # Use prediction horizons from df_scores\n",
    "# df_scores = static_featuresets[1].df\n",
    "\n",
    "# df_combined = df_dem_med.merge(df_scores, on=['PtID'], how='outer')\n",
    "# df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static_featuresets = [\n",
    "#     features.Featureset(\n",
    "#         df=df_combined, \n",
    "#         name=' + '.join([fs.name for fs in static_featuresets]), \n",
    "#         id_col = dataset.id_col,\n",
    "#         nominal_cols = [col for col in df_dem_med.columns if col != dataset.id_col]\n",
    "#     )\n",
    "# ]\n",
    "# static_featuresets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic (Temporal) Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract temporal features by converting main dataset's df from wide-form to long-form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "# Get a list of all date columns\n",
    "date_cols = list(dataset.df.filter(regex='date\\d{3}$').columns)\n",
    "\n",
    "i = 0\n",
    "for col in date_cols:\n",
    "\n",
    "    # Find all the time cols for that date col\n",
    "    time_cols = list(dataset.df.filter(\n",
    "        regex='MEMS_{date_col}_time\\d{{1}}$'.format(date_col=col)).columns)  \n",
    "\n",
    "    ''' Perform a melt so we get MEMS events stratified by patient\n",
    "        Be sure to include the \"within range\" column as one of the id_vars''' \n",
    "    additional_cols = [\n",
    "        {\n",
    "            'original': 'MEMS_' + col + '_numtimes',\n",
    "            'new': 'num_times_used_today'\n",
    "        }\n",
    "    ]\n",
    "    if i > 0: # The first date won't have an interval or withinrange\n",
    "        additional_cols.append(\n",
    "            {\n",
    "                'original': 'MEMS_' + col + '_interval',\n",
    "                'new': 'interval'\n",
    "            }\n",
    "        )\n",
    "        additional_cols.append(\n",
    "            {\n",
    "                'original': 'MEMS_' + col + '_withinrange',\n",
    "                'new': 'withinrange'\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    all_id_col = [dataset.id_col, 'DateEnroll', col] + [x['original'] for x in additional_cols]\n",
    "    \n",
    "    res = dataset.df[all_id_col + time_cols].melt(id_vars = all_id_col)\n",
    "    \n",
    "    # Tidy up the resulting dataframe\n",
    "    res.rename(columns={col: 'date', 'value': 'time', 'variable': 'MEMS_day'}, \n",
    "               inplace=True)\n",
    "\n",
    "    res['MEMS_day'] =  res['MEMS_day'].apply(lambda x: int(re.sub(r'_time\\d*$', '', x.split('MEMS_date')[1])))\n",
    "    \n",
    "    res.rename(columns={x['original']:x['new'] for x in additional_cols},\n",
    "               inplace=True)\n",
    "\n",
    "#     res.drop(columns=['variable'], inplace=True)\n",
    "    \n",
    "    rows.append(res) # TODO - double check this...getting a weird warning about index alignment\n",
    "    i += 1\n",
    "\n",
    "horizons_df = pd.concat(rows, axis=0)\n",
    "\n",
    "# Create combined datetime column\n",
    "horizons_df['datetime'] = horizons_df.apply(\n",
    "    lambda x: features.get_datetime_col(x), axis=1\n",
    ")\n",
    "horizons_df['datetime'] = pd.to_datetime(horizons_df['datetime'], errors='coerce')\n",
    "\n",
    "# Fix dtypes\n",
    "horizons_df[['withinrange', 'num_times_used_today']] = horizons_df[['withinrange', 'num_times_used_today']].fillna(0).astype(int)\n",
    "horizons_df['date'] = pd.to_datetime(horizons_df['date'], errors='coerce')\n",
    "horizons_df['interval'] = pd.to_timedelta(horizons_df['interval']) # Handle NaT intervals for first day?\n",
    "\n",
    "'''Drop rows with an empty date column.\n",
    "  Do NOT drop empty time columns - may have dates where it is recorded that the patient\n",
    "  did not use the cap. So, would have a date but no time. Need this info to calculate\n",
    "  additional stats later\n",
    "''' \n",
    "horizons_df.dropna(subset=['date'], inplace=True)\n",
    "\n",
    "# Drop duplicates - these must have been introduced with the melt and with how Kristi's original data was structured\n",
    "horizons_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Remove observations that occurred before a subject's enrollment date\n",
    "horizons_df = horizons_df.loc[horizons_df['DateEnroll'] < horizons_df['date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add binary indicator of any usage (not just number of times used) on a given day\n",
    "horizons_df['used_today'] = horizons_df['num_times_used_today'].apply(\n",
    "    lambda x: 1 if x > 0 else 0\n",
    ")\n",
    "\n",
    "'''Generate horizons of interest (time of day, weekday, day/month of study, etc)\n",
    "   'time_of_day' category gets automatically encoded as a Categorical\n",
    "''' \n",
    "horizons_df = features.get_temporal_feats(df=horizons_df, start_date_col='DateEnroll', \n",
    "                                          id_col='PtID', time_of_day_props=consts.TIME_OF_DAY_PROPS)\n",
    "\n",
    "# washout period of 1 month, per Kristi's recommendation\n",
    "horizons_df = horizons_df[horizons_df['study_month'] > 0].reset_index(drop=True)\n",
    "horizons_df['study_month'].min() # verify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Quick fix for duplicates that are introduced...\n",
    "      not sure of a better way to do this yet'''\n",
    "df = horizons_df[horizons_df.duplicated(['PtID', 'study_day', 'interval'])]\n",
    "df = df[df['num_times_used_today'] <= 1]\n",
    "horizons_df.drop(df.index, axis=0, inplace=True)\n",
    "horizons_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizons_df.select_dtypes('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_mode(x):\n",
    "    m = x.mode()\n",
    "    \n",
    "    if isinstance(m, str):\n",
    "        return m\n",
    "    else:\n",
    "        try:\n",
    "            first_mode = m[0]\n",
    "            return first_mode\n",
    "        except Exception as e:\n",
    "            return np.nan\n",
    "\n",
    "temporal_featuresets = list()\n",
    "'''Group by our desired horizon and add standard metrics such as mean, std\n",
    "'''\n",
    "\n",
    "for horizon in consts.TARGET_HORIZONS:\n",
    "    nominal_cols = []\n",
    "    groupby_cols = [dataset.id_col, horizon]\n",
    "    \n",
    "    # Get the total number of events for the given horizon\n",
    "    df = horizons_df.groupby(groupby_cols).agg(\n",
    "        n_events=('num_times_used_today', 'sum')\n",
    "    ).reset_index()\n",
    "    \n",
    "    if horizon == 'study_day':\n",
    "        cols = ['is_weekday', 'day_of_week']\n",
    "        df2 = horizons_df[groupby_cols + cols]\n",
    "        df = df.merge(df2, on=groupby_cols, how='outer')\n",
    "        \n",
    "        # Add columns indicating if the MEMS cap was used during a given time(s) of day\n",
    "        # Basically a manual one-hot encoding while we're here\n",
    "        col = 'time_of_day'\n",
    "        df2 = horizons_df.groupby(groupby_cols)[col].value_counts().reset_index(name='count')\n",
    "        df2.rename(columns={'level_2': col}, inplace=True)\n",
    "\n",
    "        df2 = df2.pivot_table(\n",
    "            columns=col, index=['PtID', 'study_day'], values='count'\n",
    "        ).reset_index().rename_axis(None, axis=1)\n",
    "        \n",
    "        df = df.merge(df2, on=groupby_cols, how='outer')\n",
    "\n",
    "        for col in consts.TIME_OF_DAY_PROPS['labels']:\n",
    "            df[col] = pd.to_numeric(df[col].fillna(0).apply(lambda x: 1 if x > 0 else x))\n",
    "        \n",
    "        cols = {x: 'time_of_day_' + x for x in consts.TIME_OF_DAY_PROPS['labels']}\n",
    "        df.rename(columns=cols,\n",
    "                  inplace=True)\n",
    "        \n",
    "        # Add to list of nominal cols after one-hot encoding\n",
    "        nominal_cols += [col for col in cols.values()]\n",
    "        \n",
    "    else:\n",
    "        if horizon == 'study_week':\n",
    "            denom = consts.DAYS_IN_WEEK\n",
    "        else:\n",
    "            denom = consts.DAYS_IN_MONTH\n",
    "        \n",
    "        # Get standard temporal metrics\n",
    "        df2 = features.calc_standard_temporal_metrics(horizons_df, groupby_cols, 'datetime')\n",
    "        df = df.merge(df2, on=groupby_cols, how='outer')\n",
    "\n",
    "        # Calculate avg and standard deviation of number of times used\n",
    "        df2 = horizons_df.groupby(groupby_cols + ['study_day'])['num_times_used_today'].max().reset_index()\n",
    "        df2 = horizons_df.groupby(groupby_cols).agg(\n",
    "            num_daily_events_mean=('num_times_used_today', lambda x: x.sum() / denom)\n",
    "        ).reset_index()\n",
    "\n",
    "        df = df.merge(df2, on=groupby_cols, how='outer')\n",
    "\n",
    "        # Get most common time of day of event occurence\n",
    "        df2 = horizons_df.groupby(groupby_cols).agg(\n",
    "            event_time_of_day_mode=('time_of_day', get_col_mode)\n",
    "        ).reset_index()\n",
    "\n",
    "        df = df.merge(df2, on=groupby_cols, how='outer')\n",
    "        \n",
    "        # Explicitly set dtype so we can later select and one-hot encode\n",
    "        df['event_time_of_day_mode'] = df['event_time_of_day_mode'].astype('category') \n",
    "\n",
    "    # Calculate adherence rate\n",
    "    if 'day' in horizon:\n",
    "        df2 = horizons_df.groupby(groupby_cols).agg(\n",
    "            adherence_rate=('withinrange', 'max')\n",
    "        ).reset_index()\n",
    "    else:\n",
    "        df2 = horizons_df.groupby(groupby_cols + ['study_day'])['withinrange'].max().reset_index() # Max will be 1 or 0\n",
    "        df2 = df2.groupby(groupby_cols).agg(\n",
    "            adherence_rate=('withinrange', lambda x: x.sum() / denom)\n",
    "        ).reset_index()\n",
    "    \n",
    "    df = df.merge(df2, on=groupby_cols, how='outer')\n",
    "    \n",
    "    # Help pandas since it doesn't process datetimes well and introduces duplicate entries on merges\n",
    "    df = df.drop_duplicates(subset=groupby_cols)\n",
    "    \n",
    "    # Create a featureset from the resulting dataframe\n",
    "    temporal_featuresets.append(features.Featureset(df=df,\n",
    "                                                    name=horizon, #Intentional for now - using horizon as name\n",
    "                                                    id_col=dataset.id_col,\n",
    "                                                    horizon=horizon,\n",
    "                                                    nominal_cols = nominal_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "temporal_featuresets[0].df.select_dtypes('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_featuresets[1].nominal_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Collinearity and Variance Inflation Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study Day\n",
    "df = temporal_featuresets[0].df\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.select_dtypes('number')\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = df2.columns\n",
    "vif_data[\"VIF\"]  = [vif(df2.values, i) for i in range(len(df2.columns))]\n",
    "vif_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop n_events to see if we improve\n",
    "df2 = df.select_dtypes('number').drop(columns=['n_events'])\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = df2.columns\n",
    "vif_data[\"VIF\"]  = [vif(df2.values, i) for i in range(len(df2.columns))]\n",
    "vif_data\n",
    "\n",
    "# Yep - looks much better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop in the actual featureset and verify it's been dropped\n",
    "temporal_featuresets[0].df.drop(columns=['n_events'], inplace=True)\n",
    "print(temporal_featuresets[0].df.columns)\n",
    "temporal_featuresets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study Week\n",
    "df = temporal_featuresets[1].df\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.select_dtypes('number')\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = df2.columns\n",
    "vif_data[\"VIF\"]  = [vif(df2.values, i) for i in range(len(df2.columns))]\n",
    "vif_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop n_events, n_daily_events mean, and between event time mean to see if we improve\n",
    "df2 = df.select_dtypes('number').drop(columns=['n_events', 'between_event_time_mean', 'num_daily_events_mean'])\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = df2.columns\n",
    "vif_data[\"VIF\"]  = [vif(df2.values, i) for i in range(len(df2.columns))]\n",
    "vif_data\n",
    "\n",
    "# Yep - looks much better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop in the actual featureset and verify it's been dropped\n",
    "temporal_featuresets[1].df.drop(columns=['n_events', 'between_event_time_mean', 'num_daily_events_mean'], inplace=True)\n",
    "print(temporal_featuresets[1].df.columns)\n",
    "temporal_featuresets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fs in temporal_featuresets:\n",
    "    print(fs.name)\n",
    "    for col in fs.df.columns:\n",
    "        if col != fs.id_col:\n",
    "            try:\n",
    "                plt.hist(fs.df[col])\n",
    "                plt.title(col)\n",
    "                plt.show()\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check - this column should NOT be in the final set\n",
    "# static_featuresets[0].df['total_days_8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'adherent'\n",
    "\n",
    "'''Set the target columns on the temporal featuresets\n",
    "   This step should be run before doing any experiments\n",
    "''' \n",
    "for t_feats in temporal_featuresets:\n",
    "    \n",
    "    #Convert adherence_rate into a binary indicator of adherence\n",
    "    t_feats.df[target_col] = t_feats.df['adherence_rate'].apply(\n",
    "        lambda x: 1 if x > consts.ADHERENCE_THRESHOLD else 0\n",
    "    )\n",
    "    # Drop the original column\n",
    "    t_feats.df.drop(columns=['adherence_rate'], inplace=True)\n",
    "\n",
    "    # Set the target col\n",
    "    t_feats.target_col = target_col\n",
    "    \n",
    "    '''For now, treat the target as a nominal column\n",
    "      This is because, if we use lagged values of the target (e.g., adherent),\n",
    "         those columns will be considered nominal, and we'll need to use the target col as a name\n",
    "         against which to find and check those columns.\n",
    "      There's a safeguard in place later, in the get_lagged_featureset function, \n",
    "        to ensure the target itself is NOT included in the final list of nominal columns.'''\n",
    "    t_feats.nominal_cols += [target_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study 1: Predict Adherence from Demographic and Med Record Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not feasible for surveys at current timescale (i.e., not taken at the same timescale as the temporal data)\n",
    "\n",
    "# combined_featuresets = []\n",
    "\n",
    "# for fs_temporal, fs_static in [(fs_temporal, fs_static) for fs_temporal in temporal_featuresets for fs_static in static_featuresets]:\n",
    "    \n",
    "#     df_static = fs_static.df.copy()\n",
    "#     df_static.drop(columns=[col for col in df_static.columns if 'study_' in col and col != fs_temporal.horizon], inplace=True)\n",
    "#     cols_expl_og = [col for col in df_static if col != fs_static.id_col and col != fs_temporal.horizon]\n",
    "    \n",
    "#     df_combined = df_static.merge(fs_temporal.df, on=[fs_temporal.id_col, fs_temporal.horizon], how='left')\n",
    "#     df_combined.dropna(subset=['adherent'], how='any', inplace=True)\n",
    "#     df_combined.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "#     fs = features.Featureset(\n",
    "#             df=df_combined, \n",
    "#             name=' + '.join([fs_static.name, fs_temporal.name]), \n",
    "#             id_col = dataset.id_col,\n",
    "#             nominal_cols = fs_static.nominal_cols + fs_temporal.nominal_cols,\n",
    "#             target_col=fs_temporal.target_col\n",
    "#             horizon=fs_temporal.horizon\n",
    "#             )\n",
    "    \n",
    "#     # One hot encode, etc\n",
    "#     fs.prep_for_modeling() # May need to reduce collinearity\n",
    "    \n",
    "#     # Create list of explanatory features\n",
    "#     to_exclude = list(fs_temporal.df.columns) \n",
    "#     to_exclude.extend(\n",
    "#         [col for col in list(fs.df.columns)\n",
    "#          if any(\n",
    "#              [cat for cat in list(fs_temporal.df.select_dtypes('category').columns) if cat in col]\n",
    "#          )\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     cols_expl = list(set(fs.df.columns) - set(to_exclude))\n",
    "    \n",
    "#     # Add combined featureset to list\n",
    "#     combined_featuresets.append(\n",
    "#         {'fs': fs,\n",
    "#          'feats_explanatory': [col for col in cols_expl \n",
    "#                                if col in fs.df.columns \n",
    "#                                and col != fs.id_col \n",
    "#                                and col != fs.horizon]\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "# combined_featuresets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study 2: Predict Adherence from MEMS Data Only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune number of lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Test the model performance for a range of lags (number of previous inputs)\n",
    "      and range of max_depths (since training with RF by default)\n",
    "    max_depth exploration will help ensure we aren't overfitting.\n",
    "'''\n",
    "# for t_feats in temporal_featuresets:\n",
    "#     models.tune_lags(t_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for f in consts.OUTPUT_PATH_LAGS.glob('*_pred.csv'):\n",
    "    df = pd.read_csv(f)\n",
    "    results.append(df)\n",
    "results = pd.concat(results, axis=0).reset_index(drop=True)\n",
    "results.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['n_lags', 'accuracy', 'max_depth']:\n",
    "    results[col] = pd.to_numeric(results[col])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['specificity_loss'] = 1-results['specificity']\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# Takeaways:\n",
    "\n",
    "# - study_day: 4 lags with max_depth 2 best for study_day - some overfitting, but not dramatic. \n",
    "# Overfitting worsens beyond this.\n",
    "\n",
    "# - study_week: 4 lags with shallow tree (max_depth=1) best for study week. \n",
    "# Overfitting worsens after that, with no major gain in performance\n",
    "# '''\n",
    "\n",
    "for max_depth in range(1, 6):\n",
    "    df = results[results['max_depth'] == max_depth]\n",
    "    \n",
    "    g = sns.lineplot(x='n_lags',y='specificity', hue='featureset', style='type', data=df)\n",
    "    g.set( ylim=(0.5, 0.8), title='Max Depth: ' + str(max_depth), ylabel='Specificity')\n",
    "    plt.show()\n",
    "    \n",
    "for max_depth in range(1, 6):\n",
    "    df = results[results['max_depth'] == max_depth]\n",
    "    \n",
    "    g = sns.lineplot(x='n_lags',y='specificity_loss', hue='featureset', style='type', data=df)\n",
    "    g.set( ylim=(0, 0.5), title='Max Depth: ' + str(max_depth), ylabel='Specificity Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----- Now predict using optimal number of lags for each horizon--- \n",
    "for t_feats in temporal_featuresets:    \n",
    "    \n",
    "    # TODO: Set RF max-depth here after tuning lags - Done\n",
    "    if t_feats.horizon == 'study_day':\n",
    "        n_lags = 4\n",
    "        max_depth = 2\n",
    "    else:\n",
    "        n_lags = 4\n",
    "        max_depth = 1\n",
    "        \n",
    "    fs_lagged = t_feats.prep_for_modeling(n_lags)\n",
    "    models.predict_from_mems(t_feats, n_lags, max_depth=max_depth)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static_featuresets[1].df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bcpn_mems",
   "language": "python",
   "name": "bcpn_mems"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
